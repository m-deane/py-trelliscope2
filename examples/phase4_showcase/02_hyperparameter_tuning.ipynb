{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Hyperparameter Tuning Grid Search\n",
    "\n",
    "**Phase 4 Features Showcased:**\n",
    "- \u2705 Performance Optimization (100+ panels, loading states)\n",
    "- \u2705 Multi-Range Filtering (cv_score > 0.8 AND fit_time < 60s)\n",
    "- \u2705 Multi-Column Sorting (score DESC, then time ASC)\n",
    "- \u2705 Label Configuration (show only critical metrics)\n",
    "- \u2705 Views (save \"Production Candidates\")\n",
    "- \u2705 Keyboard Navigation (quick browsing)\n",
    "- \u2705 Export (top 10 configurations)\n",
    "\n",
    "## Use Case\n",
    "\n",
    "Visualize results from hyperparameter grid search across Random Forest, XGBoost, and LightGBM models. Each panel shows learning curves and performance metrics for a specific hyperparameter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from trelliscope import Display\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Hyperparameter Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learning_curve_plot(train_scores, val_scores, params_str, cv_score, model_type):\n",
    "    \"\"\"Create learning curve visualization.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    iterations = np.arange(len(train_scores))\n",
    "    \n",
    "    # Learning curves\n",
    "    ax1.plot(iterations, train_scores, label='Train', color='#27AE60', linewidth=2.5, alpha=0.9)\n",
    "    ax1.plot(iterations, val_scores, label='Validation', color='#E74C3C', linewidth=2.5, alpha=0.9)\n",
    "    ax1.axhline(y=cv_score, color='#3498DB', linestyle='--', linewidth=2, label='Final CV Score', alpha=0.7)\n",
    "    ax1.set_xlabel('Iteration', fontsize=11)\n",
    "    ax1.set_ylabel('Score (R\u00b2)', fontsize=11)\n",
    "    ax1.set_title(f'{model_type} Learning Curve', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(loc='lower right', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.25, linestyle=':')\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Overfitting analysis\n",
    "    gap = train_scores - val_scores\n",
    "    ax2.plot(iterations, gap, color='#9B59B6', linewidth=2.5, alpha=0.9)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.3)\n",
    "    ax2.fill_between(iterations, 0, gap, where=(gap>0), color='#9B59B6', alpha=0.2, label='Overfitting')\n",
    "    ax2.set_xlabel('Iteration', fontsize=11)\n",
    "    ax2.set_ylabel('Train - Val Score', fontsize=11)\n",
    "    ax2.set_title('Overfitting Analysis', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(loc='upper right', fontsize=10)\n",
    "    ax2.grid(True, alpha=0.25, linestyle=':')\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    \n",
    "    fig.suptitle(f'Hyperparameters: {params_str}\\nCV Score: {cv_score:.4f}', \n",
    "                 fontsize=11, y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Define hyperparameter grids\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 200, 500],\n",
    "        'max_depth': [5, 10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.3]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'num_leaves': [15, 31, 63]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Generating hyperparameter tuning results...\")\n",
    "print(f\"RandomForest: {4 * 5 * 3} = {4*5*3} combinations\")\n",
    "print(f\"XGBoost: {3 * 4 * 4} = {3*4*4} combinations\")\n",
    "print(f\"LightGBM: {3 * 4 * 3 * 3} = {3*4*3*3} combinations\")\n",
    "print(f\"Total: {60 + 48 + 108} = {60+48+108} panels\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "data_rows = []\n",
    "panel_count = 0\n",
    "\n",
    "for model_type, param_grid in param_grids.items():\n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = [param_grid[name] for name in param_names]\n",
    "    \n",
    "    for param_combo in itertools.product(*param_values):\n",
    "        params = dict(zip(param_names, param_combo))\n",
    "        \n",
    "        # Simulate model performance based on parameters\n",
    "        # More estimators + moderate depth = better performance\n",
    "        base_score = 0.70\n",
    "        n_est_bonus = min(params.get('n_estimators', 100) / 500, 0.15)\n",
    "        depth_factor = params.get('max_depth', 10)\n",
    "        if depth_factor is None:\n",
    "            depth_factor = 20\n",
    "        depth_bonus = min(depth_factor / 100, 0.10) if depth_factor < 15 else max(0, 0.10 - (depth_factor - 15) * 0.01)\n",
    "        \n",
    "        lr = params.get('learning_rate', 0.1)\n",
    "        lr_bonus = 0.05 if 0.05 <= lr <= 0.1 else -0.02\n",
    "        \n",
    "        cv_score = base_score + n_est_bonus + depth_bonus + lr_bonus + np.random.normal(0, 0.03)\n",
    "        cv_score = np.clip(cv_score, 0.60, 0.96)\n",
    "        \n",
    "        # Training score (always >= cv_score)\n",
    "        train_score = cv_score + np.random.uniform(0.02, 0.12)\n",
    "        train_score = np.clip(train_score, cv_score, 0.99)\n",
    "        \n",
    "        # Fit time based on complexity\n",
    "        fit_time = params.get('n_estimators', 100) * 0.05\n",
    "        fit_time *= (depth_factor / 10) if depth_factor else 2.0\n",
    "        fit_time *= (1 + np.random.uniform(-0.2, 0.2))\n",
    "        \n",
    "        # Generate learning curves\n",
    "        n_iterations = params.get('n_estimators', 100)\n",
    "        train_scores = np.linspace(0.5, train_score, n_iterations) + np.random.normal(0, 0.02, n_iterations).cumsum() * 0.01\n",
    "        val_scores = np.linspace(0.5, cv_score, n_iterations) + np.random.normal(0, 0.015, n_iterations).cumsum() * 0.01\n",
    "        train_scores = np.clip(train_scores, 0.5, 0.99)\n",
    "        val_scores = np.clip(val_scores, 0.5, cv_score)\n",
    "        \n",
    "        # Create params string\n",
    "        params_str = ', '.join([f\"{k}={v}\" for k, v in params.items()])\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = create_learning_curve_plot(train_scores, val_scores, params_str, cv_score, model_type)\n",
    "        \n",
    "        # Find best params per model\n",
    "        is_best = False  # Will set after generating all\n",
    "        \n",
    "        data_rows.append({\n",
    "            'panel': fig,\n",
    "            'model_type': model_type,\n",
    "            'n_estimators': params.get('n_estimators', 100),\n",
    "            'max_depth': params.get('max_depth', 10) if params.get('max_depth') is not None else 999,\n",
    "            'learning_rate': params.get('learning_rate', 0.1),\n",
    "            'cv_score': cv_score,\n",
    "            'train_score': train_score,\n",
    "            'fit_time': fit_time,\n",
    "            'overfitting_gap': train_score - cv_score,\n",
    "            'params_str': params_str,\n",
    "            'is_best': 'no'  # Will update\n",
    "        })\n",
    "        \n",
    "        panel_count += 1\n",
    "        if panel_count % 50 == 0:\n",
    "            print(f\"  Generated {panel_count}/216 panels...\")\n",
    "        \n",
    "        plt.close(fig)\n",
    "\n",
    "print(f\"\\n\u2713 Generated {panel_count} hyperparameter configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mark Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_rows)\n",
    "\n",
    "# Mark best params per model\n",
    "for model in df['model_type'].unique():\n",
    "    mask = df['model_type'] == model\n",
    "    best_idx = df[mask]['cv_score'].idxmax()\n",
    "    df.loc[best_idx, 'is_best'] = 'yes'\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"\\nBest parameters per model:\")\n",
    "for model in df['model_type'].unique():\n",
    "    best = df[(df['model_type'] == model) & (df['is_best'] == 'yes')].iloc[0]\n",
    "    print(f\"  {model}: CV={best['cv_score']:.4f}, Time={best['fit_time']:.1f}s\")\n",
    "    print(f\"    {best['params_str']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Trelliscope Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = (\n    Display(df, name=\"hyperparameter_tuning_grid_search\", description=\"Hyperparameter grid search results for RandomForest, XGBoost, and LightGBM with 216 parameter combinations\")\n    .set_panel_column(\"panel\")\n    .infer_metas()\n    .set_default_layout(ncol=4, nrow=3)  # 12 panels per page for large dataset\n    .set_default_labels([\"model_type\", \"cv_score\", \"fit_time\", \"is_best\"])\n    # Best score, fastest time\n    .write()\n)\n\nprint(\"\\n\u2713 Trelliscope display created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launch Interactive Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trelliscope.dash_viewer import create_dash_app\n",
    "\n",
    "app = create_dash_app(display)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83d\ude80 LAUNCHING INTERACTIVE VIEWER - LARGE DATASET DEMO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n\ud83d\udcca Display: {display.name}\")\n",
    "print(f\"\ud83d\udcc8 Total Panels: {len(df)} (LARGE DATASET)\")\n",
    "print(f\"\ud83e\udd16 Models: {', '.join(df['model_type'].unique())}\")\n",
    "print(f\"\\n\u26a1 Performance Features:\")\n",
    "print(f\"  - Loading states on filter/sort operations\")\n",
    "print(f\"  - Efficient rendering for 200+ panels\")\n",
    "print(f\"  - Cached operations\")\n",
    "print(\"\\n\ud83c\udf10 Opening browser on http://localhost:8053...\\n\")\n",
    "\n",
    "app.run(debug=False, host='127.0.0.1', port=8053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Testing Guide\n",
    "\n",
    "### \u2705 Performance Optimization (Feature 3) - PRIMARY FOCUS\n",
    "\n",
    "**Dataset**: 216 panels (large enough to see loading states)\n",
    "\n",
    "**Try This**:\n",
    "1. **Initial Load**:\n",
    "   - Observe loading spinner during initial render\n",
    "   - Check browser DevTools \u2192 Performance tab\n",
    "   - Should load in < 3 seconds\n",
    "\n",
    "2. **Filter Operations**:\n",
    "   - Apply cv_score range: min=0.80, max=1.0\n",
    "   - Observe loading state appears\n",
    "   - Operation should complete in < 500ms\n",
    "\n",
    "3. **Multi-Filter Stress Test**:\n",
    "   - Add: cv_score > 0.85\n",
    "   - Add: fit_time < 50\n",
    "   - Add: overfitting_gap < 0.10\n",
    "   - Observe loading states on each\n",
    "\n",
    "4. **Sort Large Dataset**:\n",
    "   - Sort by cv_score descending\n",
    "   - Observe loading state\n",
    "   - Add second sort: fit_time ascending\n",
    "   - Observe re-sorting loading state\n",
    "\n",
    "5. **Page Navigation**:\n",
    "   - Use \u2192 arrow key to navigate pages quickly\n",
    "   - Should smoothly load next 12 panels\n",
    "   - No lag or freeze\n",
    "\n",
    "**Expected**: Loading spinners visible, operations < 500ms, no browser freeze\n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Multi-Range Filtering\n",
    "\n",
    "**Try This**:\n",
    "1. **Production-Ready Filter**:\n",
    "   - cv_score: min=0.85 (high accuracy)\n",
    "   - fit_time: max=60 (fast training)\n",
    "   - overfitting_gap: max=0.08 (low overfitting)\n",
    "   - Observe: ~10-20 panels remain\n",
    "\n",
    "2. **Best-of-Best Filter**:\n",
    "   - is_best: \"yes\"\n",
    "   - Observe: Only 3 panels (one per model)\n",
    "\n",
    "3. **Model-Specific Filter**:\n",
    "   - model_type: \"XGBoost\"\n",
    "   - n_estimators: 200\n",
    "   - learning_rate: 0.1\n",
    "   - Observe: Specific configuration subset\n",
    "\n",
    "**Expected**: Complex filters combine correctly, empty state if no matches\n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Multi-Column Sorting\n",
    "\n",
    "**Try This**:\n",
    "1. **Primary: Best Score, Secondary: Fastest**:\n",
    "   - Sort 1: cv_score (descending)\n",
    "   - Sort 2: fit_time (ascending)\n",
    "   - Result: Best performing models first, ties broken by speed\n",
    "\n",
    "2. **Primary: Model, Secondary: Score**:\n",
    "   - Sort 1: model_type (ascending) - alphabetical\n",
    "   - Sort 2: cv_score (descending)\n",
    "   - Result: Models grouped, best within each model\n",
    "\n",
    "3. **Three-Level Sort**:\n",
    "   - Sort 1: model_type (ascending)\n",
    "   - Sort 2: n_estimators (descending)\n",
    "   - Sort 3: cv_score (descending)\n",
    "   - Result: Complex hierarchical ordering\n",
    "\n",
    "**Expected**: Sorts apply in order, lower priority breaks ties\n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Views for Complex States\n",
    "\n",
    "**Try This**:\n",
    "1. **Create \"Production Candidates\" View**:\n",
    "   - Filter: cv_score > 0.85, fit_time < 60, overfitting_gap < 0.08\n",
    "   - Sort: cv_score DESC, fit_time ASC\n",
    "   - Labels: model_type, cv_score, fit_time, is_best\n",
    "   - Save as \"Production Candidates\"\n",
    "\n",
    "2. **Create \"Fast Models\" View**:\n",
    "   - Filter: fit_time < 30\n",
    "   - Sort: fit_time ASC\n",
    "   - Save as \"Fast Models\"\n",
    "\n",
    "3. **Switch Between Views**:\n",
    "   - Load \"Production Candidates\" \u2192 see high-quality configs\n",
    "   - Load \"Fast Models\" \u2192 see speedy configs\n",
    "   - Clear all \u2192 see full dataset\n",
    "\n",
    "**Expected**: Views restore complete state including multi-filter/sort\n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Keyboard Navigation (Feature 4)\n",
    "\n",
    "**With 216 Panels**:\n",
    "- **\u2192 / \u2190**: Navigate through 18 pages (12 panels each)\n",
    "- **/**: Quick search for specific model or params\n",
    "- **Esc**: Clear search\n",
    "- **\u2328\ufe0f button**: View all shortcuts\n",
    "\n",
    "**Try This**:\n",
    "1. Press \u2192 5 times quickly\n",
    "2. Observe smooth page transitions\n",
    "3. Press \u2190 to go back\n",
    "4. Press / and search \"XGBoost\"\n",
    "5. Press Esc to clear\n",
    "\n",
    "**Expected**: Responsive keyboard controls, no lag\n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Export Top Configurations\n",
    "\n",
    "**Try This**:\n",
    "1. **Export Top 10**:\n",
    "   - Sort: cv_score DESC\n",
    "   - Filter: cv_score > 0.88 (approximately top 10)\n",
    "   - Click \"Export Data (CSV)\"\n",
    "   - Open CSV: should have ~10 rows with best configs\n",
    "\n",
    "2. **Export Production View**:\n",
    "   - Load \"Production Candidates\" view\n",
    "   - Export Data (CSV) - filtered set only\n",
    "   - Export View (JSON) - save filter specification\n",
    "\n",
    "3. **Share Configuration**:\n",
    "   - Export Config (JSON)\n",
    "   - Contains full display metadata\n",
    "   - Can be used to recreate display\n",
    "\n",
    "**Expected**: CSV has filtered data, JSON has complete state\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Performance Benchmarks\n",
    "\n",
    "**Expected Timings** (216 panels):\n",
    "- Initial load: < 3s\n",
    "- Single filter: < 400ms\n",
    "- Multi-filter (3+): < 600ms\n",
    "- Sort operation: < 400ms\n",
    "- Multi-sort (3 levels): < 600ms\n",
    "- Search: < 200ms\n",
    "- Page navigation: < 300ms\n",
    "\n",
    "**How to Measure**:\n",
    "1. Open DevTools (F12)\n",
    "2. Go to Performance tab\n",
    "3. Click Record\n",
    "4. Perform operation\n",
    "5. Stop recording\n",
    "6. Check duration\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Key Insights to Discover\n",
    "\n",
    "1. **Best Overall Configuration**:\n",
    "   - Filter: is_best = \"yes\"\n",
    "   - Sort: cv_score DESC\n",
    "   - Answer: Which model + params win?\n",
    "\n",
    "2. **Speed vs Accuracy Trade-off**:\n",
    "   - Plot conceptually: cv_score vs fit_time\n",
    "   - Filter: cv_score > 0.85, fit_time < 40\n",
    "   - Answer: Sweet spot configurations\n",
    "\n",
    "3. **Overfitting Analysis**:\n",
    "   - Sort: overfitting_gap DESC\n",
    "   - See which configs overfit most\n",
    "   - Filter: overfitting_gap < 0.05\n",
    "   - See well-generalized configs\n",
    "\n",
    "4. **Model Comparison**:\n",
    "   - Group by model_type\n",
    "   - Compare best of each\n",
    "   - Answer: Which model family performs best?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This example demonstrates:\n",
    "- \u2705 Performance optimization for large datasets (200+ panels)\n",
    "- \u2705 Complex multi-range filtering\n",
    "- \u2705 Multi-column hierarchical sorting\n",
    "- \u2705 Views for saving complex analysis states\n",
    "- \u2705 Keyboard navigation for efficient browsing\n",
    "- \u2705 Export for sharing top configurations\n",
    "\n",
    "**Next**: Try Example 3 (CV Fold Analysis) for modal navigation features!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}